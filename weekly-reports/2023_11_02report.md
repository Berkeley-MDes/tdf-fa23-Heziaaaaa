# Report 10 - Week of 11/02/2023 #

### ðŸ‘‹ Hi. Welcome to the documentation of Mini Yuhe's birth log, a digital counterpart and creative extension of Yuhe Niu. This log serves as a journey through Mini Yuhe's inception, development, and evolution, offering insights into the application of ZeroWidth and LLMs.

---
## ðŸ”—. Links
Link to Youtube Video: https://youtu.be/mBVrlUTcI78

Link to demonstration: https://zerowidth.ai/c/demo/IUlUqROhfn55tZi98CY0/draft

---
## ðŸ§© . Structure

In order to make the following demonstration easier to understand. I want to introduce the structure of my LLM first.

Here is the screenshot of my ZeroWidth Workspace. There are one Intelligence named ` NNâ€™s Journal Log`, and there are three Knowledge Sets called `NN_Info` ,`NNâ€™s Project 1`, and `NNâ€™s Project 2` . 


In my `NNâ€™s Journal Log` Intelligence, I've organized four instructions, including a welcome message, an overview about me (with a linked 'name' variable), and brief introductions to Project 1 and Project 2. 

   <p align="center">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-9/mainpage.png" width="700">
   </p>

In my `NNâ€™s Journal Log` Intelligence, I've organized four instructions, including a welcome message, an overview about me (with a linked 'name' variable), and brief introductions to Project 1 and Project 2. 

   <p align="center">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-9/instruction.png" width="700">
   </p>

Complementing this, I've established three distinct knowledge sets: one detailing my personal information (`NN_Info`), and the other two each providing around 10 specific pieces of information about Projects 1 and 2, respectively (`NNâ€™s Project 1`, and `NNâ€™s Project 2`). Each information is basically a sub-section of my PDF project report. By using this methods, I successfully limited the tokens of each information to around 250-300 tokens, which also turned out to work perfectly fine in the demoâ€™s API (the Chat Bot never gave me error of using too many tokens in demo).

   <p align="center">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-9/Information.png" width="700">
   </p>

These knowledge sets are directly linked to the corresponding instructions for seamless integration. The following image shows the whole picture of my structure.

   <p align="center">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-9/Cover Page.jpg" width="700">
   </p>

---
## ðŸ¤¯ Challenge Level 

Challenge Level: 01-Triceratops
I chose the simplest level because this is a rather short project and LLM is the most unfamiliar topic for me so far. The challenges I encountered during this project are:
1. Effectively structure the database within limited tokens
2. Navigate between Intelligence and Knowledge Set and understand their relationship
3. Adjusting variables such as similarity threshold and temperature to control the output of Chat Bot and how much information was correctly pulled out

---

## ðŸ“„. Documentations of the Process

I have explored 4 aspects of LLMs parameters in ZeroWidth:
1. The construction of information in Knowledge Set
2. How to better guide user in API to ask the right questions by constructing the welcome message and other instructions right
3. Adjust Similarity Threshold
4. Adjust Temperature

### 1.  Construct & Link Knowledge Set

I had experimented with two options to input the information in Knowledge Set.
The first involved manually entering each piece of data, which I did for the basic information knowledge set (`NN_Info`).

   <p align="center">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-9/infomation_2.png" width="700">
   </p>

The second option was to upload the PDF reports containing all the details of the two projects and let ZeroWidth automatically generate the information pieces. 

   <p align="center">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-9/information_3.png" width="700">
  <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-9/information_4.png" width="700">
   </p>

However, I found this function less intuitive because it divided the information based on the PDF's page breaks. Additionally, since the PDFs included many pictures that wouldn't be imported as information pieces, some pieces turned out extremely short. Consequently, I had to manually adjust the information after importing the PDFs. After the adjustment, I got all my information laid out nicely according to the subtitle of the PDF report with reasonable of tokens and information.

   <p align="center">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-9/Instruction_P1.png" width="700">
   </p>


### 2.  Add guidance to instruction

I created the Knowledge Set to both projects and subsequently tested them in the demo. The results appeared promising. 

However, I realized that one project contained a substantial amount of information, and users might not understand its contents as well as I did. To address this, I revisited the instructions and added an index feature that would activate when a user first inquired about the project. 

   <p align="center">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-9/instruction_1.png" width="700">
   </p>

Here's how it played out in the demo.

   <p align="center">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-9/demo_1.png" width="700">
   </p>

### 3. Similarity Threshold

An unexpected incident occurred when I attempted to inquire about the challenge level I had chosen for Project 1. Surprisingly, the bot responded that it lacked access to this specific piece of information. 

   <p align="center">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-9/similarity_1.png" width="700">
   </p>
   
Perplexed by this response, I revisited the knowledge set and indeed found information about the challenge level. 

   <p align="center">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-9/similarity_2.png" width="700">
   </p>

After some exploration and investigation, I realized that I had initially set the similarity threshold to such a high level that it was preventing the bot from retrieving the correct information. 

   <p align="center">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-9/similarity_3.png" width="700">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-9/similarity_4.png" width="700">
   </p>

Once I adjusted the threshold to a lower level, the bot successfully answered the question regarding the challenge level.

   <p align="center">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-9/similarity_6.png" width="700">
   </p>


### 4.  Temperature

After experimenting with the settings in ZeroWidth, I encountered an issue where the bot mixed up details from the Project 2 knowledge set when inquiring about what tools I used for Project 1.  (I didnâ€™t used Figma for Project 1. Instead, I used it for Project 2)

   <p align="center">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-9/temperature_1.png" width="700">
   </p>

To rectify this, I adjusted the temperature, increased both the Frequency and Presence Penalties, and upon testing in the demo, the bot now accurately pulls information from the correct knowledge set.

   <p align="center">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-9/temperature_2.png" width="350">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-9/temperature_3.png" width="350">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-9/temperature_4.png" width="700">
   </p>

---
## Speculations

When speculating about the potential applications of LLM (Large Language Models), the first idea that came to mind was GitHub Copilot. As someone relatively new to coding, I've experienced firsthand how Copilot serves as a powerful tool, significantly reducing the time I would otherwise spend searching online for coding solutions. Extrapolating this concept to other domains of specialization holds the promise of drastically lowering the learning curve when acquiring new skill sets. It also has the potential to blur the boundaries between different fields of specialization, thereby fostering interdisciplinary collaboration and innovation

   <p align="center">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-9/2.jpg" width="700">
   </p>
   
Large Language Models (LLMs) have the potential to revolutionize TV entertainment by crafting personalized shows tailored to individual viewers. Leveraging the fluid processing capabilities of LLMs, TV content could dynamically adjust to match each viewer's preferences, generating storylines and characters that deeply resonate with their tastes and emotions. This transformation would shift traditional passive viewing into a bespoke and interactive experience, where shows continuously evolve to align with the unique desires and interests of every audience member.

   <p align="center">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-9/3.jpg" width="700">
   </p>


---
## Conclusion

In conclusion, the presentation of prototypes as digital twins carries immense potential for innovation but equally demands stringent ethical considerations. Among these considerations, safeguarding privacy, ensuring transparency, and maintaining data accuracy are paramount.
Digital twins serve as efficient tools, bridging geographical and temporal gaps, allowing people to understand and interact with complex concepts or individuals within limited constraints. However, it's important to acknowledge that no digital twin can ever fully represent the dynamic nature of humans. Our identities are constantly evolving, shaped by experiences, and influenced by the ever-changing world around us. We are more than mere binary data sets; we are nuanced, multifaceted beings.

As we embrace digital twin technology, it becomes increasingly crucial to strike a balance between innovation and ethical responsibility. We must harness the technology's potential while respecting the intricacies of human identity and acknowledging that our thoughts, values, and perspectives change over time. Nurturing a sense of humility in the face of our ever-evolving selves is an essential aspect of responsible digital twin development, ensuring that these tools are used to empower and inform, rather than to reduce individuals to static, simplistic representations.
