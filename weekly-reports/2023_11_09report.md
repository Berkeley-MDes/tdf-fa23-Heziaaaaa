# Report 11 - Week of 11/09/2023 #

### This week, I have started brainstorming what I should do for Project 4, the final project. The following will be my initial ideation for the Project 4. 

---

## Project Overview

Project Name: Emo Projector

Challenge Level: 02-Platypus

   <p align="center">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-10/Emo Projector.png" width="700">
   </p>

---

## Project Summary

The Emo Projector is an innovative solution aimed at mitigating the loneliness experienced by individuals living alone, particularly tailored for those with an INFP/ENFP personality. This device harnesses random thoughts and words that often occur to such individuals, converting them into a unique cinematic experience.

Utilizing AI technology, the projector listens to specific keywords from the user and generates a series of stories and frame-by-frame mini-movies. This results in a customized cinematic journey, intimately aligned with the user's fleeting thoughts and solitary moments.

To enhance user interaction, the Emo Projector incorporates Photon technology, which allows for more dynamic responses to user inputs. Additionally, Large Language Models (LLMs) are employed to interpret and elaborate on the user's verbal cues. This integration ensures a seamless transition from thought to visual narrative, creating an engaging and responsive experience that not only entertains but also emotionally supports the user during times of solitude.

---

## Technology

   <p align="center">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-10/Tech.png" width="900">
   </p>

The system comprises two main components: a microcontroller known as Photon and two language models, namely ChatGPT and DALL·E. In line with my initial concept for the technological flow, the program commences with Photon's microphone functioning as a voice recorder to capture audio input. The recorded audio is subsequently transcribed into text using a speech-to-text engine powered by OpenAI. Once the audio is converted into text, it undergoes processing by a ChatGPT model, which interprets the content and generates a corresponding textual prompt for expanding on the concept. This prompt is then utilized by DALL·E, a specialized AI designed to translate text prompts into images, resulting in the creation of a visual representation. The generated image is projected onto a screen by a Projector, making it visible to the audience. Finally, a speaker narrates the generated story while the projected image complements the narration.

The technological difficulty of this system lies in the seamless integration of different AI-driven components, each with its own complexities. The voice recognition must be accurate to ensure the text reflects the audio input correctly. The ChatGPT model must generate prompts that are contextually relevant to the transcribed text. DALL·E's challenge is to interpret the prompts accurately and create images that are a close representation of the intended visual content. Finally, the synchronization of audio from the Speaker and the corresponding visual display from the Projector requires precise timing to ensure a cohesive user experience.

---

## Research Plan

### Transition of data between platforms

1. How to convert audio to text
2. How to automatically feed text into ChatGPT
3. How to automatically feed ChatGPT’s prompt to Midjourney
4. How to transport Midjourney’s images to the projector
5. How to transport ChatGPT’s storyline to speaker

### Personalization of LLMs

1. Train a specific GPT for this project with all presets
2. Train a LoRA in Stable Diffusion or train a certain style in Midjourney to generate consistent images

---

## Timeline

### Nov 13
- Form team and divide up tasks

### Nov 20
- Train a tailored GPT for this project
- Train the DALL.E prompt

### Nov 27
- Solve the transition between platfroms (audio to text, text to GPT, GPT to DALL.E, DALL.E to projector)
- Photon connection
- Design and 3D Print the body for Emo Projector

### Dec 3
- Usability Test
- Continue refining LLMs
