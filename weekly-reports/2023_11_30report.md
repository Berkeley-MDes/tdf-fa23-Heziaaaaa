# Report 14 - Week of 11/30/2023 #

## Working Process
Since I take charge of the coding part with Python, I will mainly document the programming process in the following part. The progress is divided into five steps:
1. Send the voice recording to the OpenAI’s speech-to-text generator
2. Receive the text and sending it to ChatGPT to generate the corresponding story and DALL.E prompt
3. Receive DALL.E prompt and send it to OpenAI’s DALL.E to generate the storyboard. Receive the decoded image and save it to local laptop
4. Send the story to OpenAI’s text-to-speech generator and receive the generated audio back to local laptop
5. Display the image along with the audio for the user

---

### During this week, I worked on rest of the workflow and successfully intergrated DALL.E API with Python

## Step 3: Call DALL.E’s API
The subsequent phase involves parsing the text received and generating a distinct DALL·E prompt, which is then transmitted to the DALL·E API. The image obtained in response is saved as a base64 JSON file in my local directory. This file is subsequently decoded into a JPEG format and stored in the same folder, labeled with a filename format like "str(round)+.jpeg," where "round" corresponds to the current round or iteration of the process.

  <p align="center">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-12/code_5.png" width="1000">
   </p>

  </br>

## Step 4: Receive audio from OpenAI’s text-to-speech generator
The concluding step involves sending the story prompt independently to OpenAI's built-in text-to-speech API. The audio generated in response is then saved in a separate local folder.

  <p align="center">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-12/code_6.png" width="1000">
   </p>

Finally, threading is implemented to simultaneously play the sound and display the image on my laptop. When the entire cycle is completed, a new round number is generated and stored for tracking purposes.

  <p align="center">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-12/code_7.png" width="1000">
   </p>
   
<br>

## Step 0: Syncing with Photon 2
The final step for me is to sync the audio collected by the Photon and stored in the local laptop to feed into the speech-to-text generator. Here is the code from my side.

  <p align="center">
   <img src="https://github.com/Berkeley-MDes/tdf-fa23-Heziaaaaa/blob/main/image/week-12/code_8.png" width="1000">
   </p>
